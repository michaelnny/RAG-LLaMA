{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test add alert codes as custom tokens to the pretrained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# simple hack to support import module from parent directory\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "from rag_llama.models.embedding import create_embedding_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "alert_codes = [\n",
        "    \"UMC_a013\",\n",
        "    \"CC_a005\",\n",
        "    \"BMS_a068\",\n",
        "    \"UMC_a008\",\n",
        "    \"CP_a164\",\n",
        "    \"CC_a043\",\n",
        "    \"CP_a101\",\n",
        "    \"CP_a043\",\n",
        "    \"CC_a012\",\n",
        "    \"CC_a011\",\n",
        "    \"UMC_a018\",\n",
        "    \"DI_a166\",\n",
        "    \"UMC_a016\",\n",
        "    \"CC_a018\",\n",
        "    \"CC_a028\",\n",
        "    \"DI_a184\",\n",
        "    \"APP_w009\",\n",
        "    \"UMC_a002\",\n",
        "    \"APP_w224\",\n",
        "    \"CC_a010\",\n",
        "    \"APP_w048\",\n",
        "    \"APP_w222\",\n",
        "    \"CP_a054\",\n",
        "    \"DI_a190\",\n",
        "    \"UMC_a009\",\n",
        "    \"APP_w207\",\n",
        "    \"UMC_a015\",\n",
        "    \"CP_a053\",\n",
        "    \"CC_a022\",\n",
        "    \"CP_a066\",\n",
        "    \"APP_w304\",\n",
        "    \"CC_a020\",\n",
        "    \"CP_a102\",\n",
        "    \"CC_a025\",\n",
        "    \"TAS_a313\",\n",
        "    \"CC_a003\",\n",
        "    \"BMS_a069\",\n",
        "    \"CC_a007\",\n",
        "    \"CC_a009\",\n",
        "    \"CC_a026\",\n",
        "    \"UMC_a017\",\n",
        "    \"CC_a006\",\n",
        "    \"CP_a078\",\n",
        "    \"CC_a014\",\n",
        "    \"CP_a004\",\n",
        "    \"UMC_a005\",\n",
        "    \"CC_a030\",\n",
        "    \"CC_a004\",\n",
        "    \"DI_a175\",\n",
        "    \"CC_a002\",\n",
        "    \"CC_a027\",\n",
        "    \"UMC_a007\",\n",
        "    \"CC_a016\",\n",
        "    \"UMC_a012\",\n",
        "    \"CP_a143\",\n",
        "    \"CC_a024\",\n",
        "    \"APP_w218\",\n",
        "    \"CC_a019\",\n",
        "    \"CP_a010\",\n",
        "    \"CP_a051\",\n",
        "    \"DI_a245\",\n",
        "    \"UMC_a011\",\n",
        "    \"CP_a079\",\n",
        "    \"CC_a001\",\n",
        "    \"CC_a023\",\n",
        "    \"CP_a058\",\n",
        "    \"CP_a046\",\n",
        "    \"CP_a056\",\n",
        "    \"UMC_a001\",\n",
        "    \"TAS_a314\",\n",
        "    \"UMC_a010\",\n",
        "    \"UMC_a019\",\n",
        "    \"CC_a017\",\n",
        "    \"CC_a015\",\n",
        "    \"CC_a041\",\n",
        "    \"CP_a055\",\n",
        "    \"DI_a138\",\n",
        "    \"DI_a250\",\n",
        "    \"UMC_a004\",\n",
        "    \"DI_a185\",\n",
        "    \"BMS_a066\",\n",
        "    \"CC_a008\",\n",
        "    \"CC_a029\",\n",
        "    \"CP_a151\",\n",
        "    \"BMS_a067\",\n",
        "    \"CC_a021\",\n",
        "    \"UMC_a014\",\n",
        "    \"CC_a013\"\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment_on_alert(alert, tokenizer) -> bool:\n",
        "    question = f\"what does {alert} mean\"\n",
        "    encode_output = tokenizer.tokenize(question)\n",
        "    decode_output = tokenizer.convert_tokens_to_string(encode_output)\n",
        "\n",
        "    if decode_output != question.lower():\n",
        "        return False\n",
        "    \n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading sentence-transformers/all-MiniLM-L6-v2 tokenizer from HuggingFace...\n",
            "['what', 'does', 'app', '_', 'w', '##22', '##2', 'mean']\n",
            "what does app _ w222 mean\n"
          ]
        }
      ],
      "source": [
        "tokenizer = create_embedding_tokenizer()\n",
        "question = \"what does APP_w222 mean\"\n",
        "encode_output = tokenizer.tokenize(question)\n",
        "decode_output = tokenizer.convert_tokens_to_string(encode_output)\n",
        "print(encode_output)\n",
        "print(decode_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.add_tokens(alert_codes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['what', 'does', 'app_w222', 'mean']\n",
            "what does app_w222 mean\n"
          ]
        }
      ],
      "source": [
        "question = \"what does APP_w222 mean\"\n",
        "encode_output = tokenizer.tokenize(question)\n",
        "decode_output = tokenizer.convert_tokens_to_string(encode_output)\n",
        "print(encode_output)\n",
        "print(decode_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = [run_experiment_on_alert(alert, tokenizer) for alert in alert_codes]\n",
        "assert all(results), results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make sure can load from saved tokenizer checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer from ../checkpoints/finetune_embedding/tokenizer...\n"
          ]
        }
      ],
      "source": [
        "tokenizer_ckpt_dir = '../checkpoints/finetune_embedding/tokenizer'\n",
        "saved_tokenizer = create_embedding_tokenizer(tokenizer_ckpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = [run_experiment_on_alert(alert, saved_tokenizer) for alert in alert_codes]\n",
        "assert all(results), results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
